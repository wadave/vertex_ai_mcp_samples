{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Use Gemini 2.5 Pro to create MCP Server\n",
        "\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/wadave/vertex_ai_mcp_samples/blob/main/create_mcp_server_by_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  \n",
        "  \n",
        "  \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/wadave/vertex_ai_mcp_samples/blob/main/create_mcp_server_by_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Dave Wang](https://github.com/wadave) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use Gemini 2.5 pro create MCP server codes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites\n",
        "- Get google pai key\n",
        "- Set up google api key and save it in google cloud secret manager (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai google-cloud-secret-manager mcp geopy black google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "from google import genai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "#client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import secretmanager\n",
        "from google import genai\n",
        "import datetime\n",
        "\n",
        "from google.genai.types import (\n",
        "    CreateBatchJobConfig,\n",
        "    CreateCachedContentConfig,\n",
        "    EmbedContentConfig,\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        ")\n",
        "\n",
        "import json\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "from typing import Any, List\n",
        "import asyncio\n",
        "from google.genai import types\n",
        "from typing import List\n",
        "from util import(\n",
        "    access_secret_version,\n",
        "    get_url_content,\n",
        "    format_python\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1 Get API key from secret manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO you need to create a 'google_api_key' and save it in secret manager\n",
        "api_key = access_secret_version(PROJECT_ID, secret_id=\"google_api_key\", version_id=\"latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2 Get api key from environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# api_key=os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO(developer): Update below line\n",
        "API_KEY = api_key\n",
        "\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "MODEL25_ID = \"gemini-2.5-pro-exp-03-25\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get system instruction context info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The URL you want to fetch\n",
        "url = 'https://modelcontextprotocol.io/quickstart/server'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched content\n"
          ]
        }
      ],
      "source": [
        "reference_content = get_url_content(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "class response_schema(BaseModel):\n",
        "    python_code: str\n",
        "    description: str\n",
        "    \n",
        "system_instruction = f\"\"\"\n",
        "  You are an MCP server export.\n",
        "  Your mission is to write python code for MCP server.\n",
        "  Here's the MCP server development guide and example\n",
        "  {reference_content}\n",
        "  \n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_mcp_server(prompt): \n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL25_ID,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=response_schema,\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gemini Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define gemini agent loop\n",
        "MODEL_ID = \"gemini-2.0-flash-001\"\n",
        "async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):\n",
        "    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
        "    # Initialize the connection\n",
        "    await session.initialize()\n",
        "    \n",
        "    # --- 1. Get Tools from Session and convert to Gemini Tool objects ---\n",
        "    mcp_tools = await session.list_tools()\n",
        "    tools = types.Tool(function_declarations=[\n",
        "        {\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description,\n",
        "            \"parameters\": tool.inputSchema,\n",
        "        }\n",
        "        for tool in mcp_tools.tools\n",
        "    ])\n",
        "    \n",
        "    # --- 2. Initial Request with user prompt and function declarations ---\n",
        "    response = await client.aio.models.generate_content(\n",
        "        model=MODEL_ID,  # Or your preferred model supporting function calling\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0,\n",
        "            tools=[tools],\n",
        "        ),  # Example other config\n",
        "    )\n",
        "    \n",
        "    # --- 3. Append initial response to contents ---\n",
        "    contents.append(response.candidates[0].content)\n",
        "\n",
        "    # --- 4. Tool Calling Loop ---            \n",
        "    turn_count = 0\n",
        "    max_tool_turns = 5\n",
        "    while response.function_calls and turn_count < max_tool_turns:\n",
        "        turn_count += 1\n",
        "        tool_response_parts: List[types.Part] = []\n",
        "\n",
        "        # --- 4.1 Process all function calls in order and return in this turn ---\n",
        "        for fc_part in response.function_calls:\n",
        "            tool_name = fc_part.name\n",
        "            args = fc_part.args or {}  # Ensure args is a dict\n",
        "            print(f\"Attempting to call MCP tool: '{tool_name}' with args: {args}\")\n",
        "\n",
        "            tool_response: dict\n",
        "            try:\n",
        "                # Call the session's tool executor\n",
        "                tool_result = await session.call_tool(tool_name, args)\n",
        "                print(f\"MCP tool '{tool_name}' executed successfully.\")\n",
        "                if tool_result.isError:\n",
        "                    tool_response = {\"error\": tool_result.content[0].text}\n",
        "                else:\n",
        "                    tool_response = {\"result\": tool_result.content[0].text}\n",
        "            except Exception as e:\n",
        "                tool_response = {\"error\":  f\"Tool execution failed: {type(e).__name__}: {e}\"}\n",
        "            \n",
        "            # Prepare FunctionResponse Part\n",
        "            tool_response_parts.append(\n",
        "                types.Part.from_function_response(\n",
        "                    name=tool_name, response=tool_response\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # --- 4.2 Add the tool response(s) to history ---\n",
        "        contents.append(types.Content(role=\"user\", parts=tool_response_parts))\n",
        "        print(f\"Added {len(tool_response_parts)} tool response parts to history.\")\n",
        "\n",
        "        # --- 4.3 Make the next call to the model with updated history ---\n",
        "        print(\"Making subsequent API call with tool responses...\")\n",
        "        response = await client.aio.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,  # Send updated history\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=1.0,\n",
        "                tools=[tools],\n",
        "            ),  # Keep sending same config\n",
        "        )\n",
        "        contents.append(response.candidates[0].content)\n",
        "\n",
        "    if turn_count >= max_tool_turns and response.function_calls:\n",
        "        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop.\")\n",
        "\n",
        "    print(\"MCP tool calling loop finished. Returning final response.\")\n",
        "    # --- 5. Return Final Response ---\n",
        "    return response\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1:  Create Big Query MCP Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Please create an MCP server code for google cloud big query. It has two tools. One is to list tables for all datasets, the other is to describe a table. Google cloud project id and location will be provided for use. please use project id to access big query client.\n",
        "  \n",
        "  Return Python code within a JSON object formatted exactly as: {'python_code':    'your_generated_code', 'description':'your description'}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_text= generate_mcp_server(prompt)\n",
        "python_code=json.loads(response_text)['python_code']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Format python code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully formatted the code and saved it to 'bq_script2.py'\n"
          ]
        }
      ],
      "source": [
        "format_python(python_code, \"bq_script2.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "bq_server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    # Make sure to update to the full absolute path to your weather_server.py file\n",
        "    args=[\"./bq_script2.py\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: list my big query tables, project id is 'ace-chatbot-demo', location is 'us'\n",
            "Attempting to call MCP tool: 'list_tables_for_all_datasets' with args: {'project_id': 'ace-chatbot-demo'}\n",
            "MCP tool 'list_tables_for_all_datasets' executed successfully.\n",
            "Added 1 tool response parts to history.\n",
            "Making subsequent API call with tool responses...\n",
            "MCP tool calling loop finished. Returning final response.\n",
            "Here is a list of tables in project 'ace-chatbot-demo':\n",
            "\n",
            "Dataset: aaa_demo_v0001_agents\n",
            "  - insights_v3\n",
            "  - interaction_logs\n",
            "\n",
            "Dataset: aaa_demo_v0001_app\n",
            "  - user_info\n",
            "\n",
            "Dataset: conversation_sample\n",
            "  - conversation_sample1\n",
            "  - insights\n",
            "\n"
          ]
        }
      ],
      "source": [
        "async def run():\n",
        "    async with stdio_client(bq_server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"list my big query tables, project id is 'ace-chatbot-demo', location is 'us'\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample 2: Medlineplus \n",
        "Create an MCP server for \n",
        "https://medlineplus.gov/about/developers/webservices/ API service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "med_url =\"https://medlineplus.gov/about/developers/webservices/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched content\n"
          ]
        }
      ],
      "source": [
        "med_api_details = get_url_content(med_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully formatted the code and saved it to 'med.py'\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "  Please create an MCP server code for https://medlineplus.gov/about/developers/webservices/. It has one tool, get_medical_term. You provide a medical term, this tool will return explanation of the medial term\n",
        "  \n",
        "  Here's the API details:\n",
        "  {med_api_details}\n",
        "\"\"\"\n",
        "def generate_mcp_server(prompt): \n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL25_ID,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=response_schema,\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    return response.text\n",
        "response_text= generate_mcp_server(prompt)\n",
        "python_code=json.loads(response_text)['python_code']\n",
        "\n",
        "format_python(python_code, \"med.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "med_server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    # Make sure to update to the full absolute path to your weather_server.py file\n",
        "    args=[\"./med.py\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: tell me detail of flu\n",
            "Attempting to call MCP tool: 'get_medical_term' with args: {'term': 'flu'}\n",
            "MCP tool 'get_medical_term' executed successfully.\n",
            "Added 1 tool response parts to history.\n",
            "Making subsequent API call with tool responses...\n",
            "MCP tool calling loop finished. Returning final response.\n",
            "The flu, also called influenza, is a respiratory infection caused by viruses. Symptoms include fever, cough, sore throat, runny or stuffy nose, muscle or body aches, headaches, and fatigue. Some people may also have vomiting and diarrhea. It is best to get a flu vaccine every year to prevent it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "async def run():\n",
        "    async with stdio_client(med_server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"tell me detail of flu\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_template.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
