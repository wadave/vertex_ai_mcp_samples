{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Use Gemini 2.5 Pro to create MCP Server\n",
        "\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/wadave/vertex_ai_mcp_samples/blob/main/create_mcp_server_by_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  \n",
        "  \n",
        "  \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/wadave/vertex_ai_mcp_samples/blob/main/create_mcp_server_by_gemini.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/notebook_template.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Dave Wang](https://github.com/wadave) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "The Model Context Protocol (MCP) is an open standard that streamlines the integration of AI assistants with external data sources, tools, and systems. [MCP standardizes how applications provide context to LLMs](https://modelcontextprotocol.io/introduction). MCP establishes the essential standardized interface allowing AI models to connect directly with diverse external systems and services.\n",
        "\n",
        "Developers have the option to use third-party MCP servers or create custom ones when building applications. \n",
        "\n",
        "\n",
        "This notebook demonstrates how to use Gemini 2.5 pro to create MCP servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites\n",
        "Gemini 2.5 Pro is currently only available via Google API/App or AI Studio(as of 3/30/2025)\n",
        "- Get a Google API key\n",
        "- Save the Google API as 'google_api_key' in your google cloud secret manager (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai google-cloud-secret-manager mcp geopy black google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "from google import genai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "#client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import secretmanager\n",
        "from google import genai\n",
        "import datetime\n",
        "\n",
        "from google.genai.types import (\n",
        "    GenerateContentConfig,\n",
        ")\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "import asyncio\n",
        "from google.genai import types\n",
        "from typing import List\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"/util\"))\n",
        "from util.util import(\n",
        "    access_secret_version,\n",
        "    get_url_content,\n",
        "    format_python\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1 Get Google API key from secret manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO you need to create a 'google_api_key' and save it in secret manager\n",
        "api_key = access_secret_version(PROJECT_ID, secret_id=\"google_api_key\", version_id=\"latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2 Get Google API key from environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# api_key=os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO(developer): Update below line\n",
        "API_KEY = api_key\n",
        "\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "MODEL25_ID = \"gemini-2.5-pro-exp-03-25\" #This model is only for MCP server code generation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get system instruction context info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The URL you want to fetch\n",
        "url = 'https://modelcontextprotocol.io/quickstart/server'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched content\n"
          ]
        }
      ],
      "source": [
        "reference_content = get_url_content(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up system instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "class response_schema(BaseModel):\n",
        "    python_code: str\n",
        "    description: str\n",
        "    \n",
        "system_instruction = f\"\"\"\n",
        "  You are an MCP server export.\n",
        "  Your mission is to write python code for MCP server.\n",
        "  Here's the MCP server development guide and example\n",
        "  {reference_content}\n",
        "  \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_mcp_server(prompt): \n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL25_ID,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=response_schema,\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gemini Agent\n",
        "\n",
        "Within an MCP client session, this agent loop runs a multi-turn conversation loop with a Gemini model, handling tool calls via MCP server.\n",
        "\n",
        "This function orchestrates the interaction between a user prompt, a Gemini model capable of function calling, and a session object that provides and executes tools. It handles the cycle of:\n",
        "-  Gemini gets tool information from MCP client session\n",
        "-  Sending the user prompt (and conversation history) to the model.\n",
        "-  If the model requests tool calls, Gemini makes initial function calls to get structured data as per schema, and \n",
        "-  Sending the tool execution results back to the model.\n",
        "-  Repeating until the model provides a text response or the maximum number of tool execution turns is reached.\n",
        "-  Gemini generates final response based on tool responses and original query.\n",
        "  \n",
        "MCP integration with Gemini\n",
        "\n",
        "<img src=\"asset/mcp_tool_call.png\" alt=\"MCP with Gemini\" height=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Consider using a more recent/recommended model if available and suitable\n",
        "MODEL_ID = \"gemini-2.5-pro-exp-03-25\" # You can use a different model for the agent\n",
        "DEFAULT_MAX_TOOL_TURNS = 5 # Maximum consecutive turns for tool execution\n",
        "DEFAULT_INITIAL_TEMPERATURE = 0.0 # Temperature for the first LLM call (more deterministic)\n",
        "DEFAULT_TOOL_CALL_TEMPERATURE = 1.0 # Temperature for LLM calls after tool use (potentially more creative)\n",
        "\n",
        "# Make tool calls via MCP Server\n",
        "async def _execute_tool_calls(\n",
        "    function_calls: List[types.FunctionCall],\n",
        "    session: ClientSession\n",
        ") -> List[types.Part]:\n",
        "    \"\"\"\n",
        "    Executes a list of function calls requested by the Gemini model via the session.\n",
        "\n",
        "    Args:\n",
        "        function_calls: A list of FunctionCall objects from the model's response.\n",
        "        session: The session object capable of executing tools via `call_tool`.\n",
        "\n",
        "    Returns:\n",
        "        A list of Part objects, each containing a FunctionResponse corresponding\n",
        "        to the execution result of a requested tool call.\n",
        "    \"\"\"\n",
        "    tool_response_parts: List[types.Part] = []\n",
        "    print(f\"--- Executing {len(function_calls)} tool call(s) ---\")\n",
        "\n",
        "    for func_call in function_calls:\n",
        "        tool_name = func_call.name\n",
        "        # Ensure args is a dictionary, even if missing or not a dict type\n",
        "        args = func_call.args if isinstance(func_call.args, dict) else {}\n",
        "        print(f\"  Attempting to call session tool: '{tool_name}' with args: {args}\")\n",
        "\n",
        "        tool_result_payload: Dict[str, Any]\n",
        "        try:\n",
        "            # Execute the tool using the provided session object\n",
        "            # Assumes session.call_tool returns an object with attributes\n",
        "            # like `isError` (bool) and `content` (list of Part-like objects).\n",
        "            tool_result = await session.call_tool(tool_name, args)\n",
        "            print(f\"  Session tool '{tool_name}' execution finished.\")\n",
        "\n",
        "            # Extract result or error message from the tool result object\n",
        "            result_text = \"\"\n",
        "            # Check structure carefully based on actual `session.call_tool` return type\n",
        "            if hasattr(tool_result, 'content') and tool_result.content and hasattr(tool_result.content[0], 'text'):\n",
        "                 result_text = tool_result.content[0].text or \"\"\n",
        "\n",
        "            if hasattr(tool_result, 'isError') and tool_result.isError:\n",
        "                error_message = result_text or f\"Tool '{tool_name}' failed without specific error message.\"\n",
        "                print(f\"  Tool '{tool_name}' reported an error: {error_message}\")\n",
        "                tool_result_payload = {\"error\": error_message}\n",
        "            else:\n",
        "                 print(f\"  Tool '{tool_name}' succeeded. Result snippet: {result_text[:150]}...\") # Log snippet\n",
        "                 tool_result_payload = {\"result\": result_text}\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch exceptions during the tool call itself\n",
        "            error_message = f\"Tool execution framework failed: {type(e).__name__}: {e}\"\n",
        "            print(f\"  Error executing tool '{tool_name}': {error_message}\")\n",
        "            tool_result_payload = {\"error\": error_message}\n",
        "\n",
        "        # Create a FunctionResponse Part to send back to the model\n",
        "        tool_response_parts.append(\n",
        "            types.Part.from_function_response(\n",
        "                name=tool_name, response=tool_result_payload\n",
        "            )\n",
        "        )\n",
        "    print(f\"--- Finished executing tool call(s) ---\")\n",
        "    return tool_response_parts\n",
        "\n",
        "\n",
        "async def run_agent_loop(\n",
        "    prompt: str,\n",
        "    client: genai.Client, \n",
        "    session: ClientSession,\n",
        "    model_id: str = MODEL_ID, \n",
        "    max_tool_turns: int = DEFAULT_MAX_TOOL_TURNS,\n",
        "    initial_temperature: float = DEFAULT_INITIAL_TEMPERATURE,\n",
        "    tool_call_temperature: float = DEFAULT_TOOL_CALL_TEMPERATURE\n",
        ") -> types.GenerateContentResponse:\n",
        "    \"\"\"\n",
        "    Runs a multi-turn conversation loop with a Gemini model, handling tool calls.\n",
        "\n",
        "    This function orchestrates the interaction between a user prompt, a Gemini\n",
        "    model capable of function calling, and a session object that provides\n",
        "    and executes tools. It handles the cycle of:\n",
        "    1. Sending the user prompt (and conversation history) to the model.\n",
        "    2. If the model requests tool calls, executing them via the `session`.\n",
        "    3. Sending the tool execution results back to the model.\n",
        "    4. Repeating until the model provides a text response or the maximum\n",
        "       number of tool execution turns is reached.\n",
        "\n",
        "    Args:\n",
        "        prompt: The initial user prompt to start the conversation.\n",
        "        client: An initialized Gemini GenerativeModel client object\n",
        "                \n",
        "        session: An active session object responsible for listing available tools\n",
        "                 via `list_tools()` and executing them via `call_tool(tool_name, args)`.\n",
        "                 It's also expected to have an `initialize()` method.\n",
        "        model_id: The identifier of the Gemini model to use (e.g., \"gemini-1.5-pro-latest\").\n",
        "        max_tool_turns: The maximum number of consecutive turns dedicated to tool calls\n",
        "                        before forcing a final response or exiting.\n",
        "        initial_temperature: The temperature setting for the first model call.\n",
        "        tool_call_temperature: The temperature setting for subsequent model calls\n",
        "                               that occur after tool execution.\n",
        "\n",
        "    Returns:\n",
        "        The final Response from the Gemini model after the\n",
        "        conversation loop concludes (either with a text response or after\n",
        "        reaching the max tool turns).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the session object does not provide any tools.\n",
        "        Exception: Can potentially raise exceptions from the underlying API calls\n",
        "                   or session tool execution if not caught internally by `_execute_tool_calls`.\n",
        "    \"\"\"\n",
        "    print(f\"Starting agent loop with model '{model_id}' and prompt: '{prompt[:100]}...'\")\n",
        "\n",
        "    # Initialize conversation history with the user's prompt\n",
        "    contents: List[types.Content] = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
        "\n",
        "    # Ensure the session is ready (if needed)\n",
        "    if hasattr(session, 'initialize') and callable(session.initialize):\n",
        "        print(\"Initializing session...\")\n",
        "        await session.initialize()\n",
        "    else:\n",
        "        print(\"Session object does not have an initialize() method, proceeding anyway.\")\n",
        "\n",
        "    # --- 1. Discover Tools from Session ---\n",
        "    print(\"Listing tools from session...\")\n",
        "    # Assumes session.list_tools() returns an object with a 'tools' attribute (list)\n",
        "    # Each item in the list should have 'name', 'description', and 'inputSchema' attributes.\n",
        "    session_tool_list = await session.list_tools()\n",
        "\n",
        "    if not session_tool_list or not session_tool_list.tools:\n",
        "        raise ValueError(\"No tools provided by the session. Agent loop cannot proceed.\")\n",
        "\n",
        "    # Convert session tools to the format required by the Gemini API\n",
        "    gemini_tool_config = types.Tool(function_declarations=[\n",
        "        types.FunctionDeclaration(\n",
        "            name=tool.name,\n",
        "            description=tool.description,\n",
        "            parameters=tool.inputSchema, # Assumes inputSchema is compatible\n",
        "        )\n",
        "        for tool in session_tool_list.tools\n",
        "    ])\n",
        "    print(f\"Configured Gemini with {len(gemini_tool_config.function_declarations)} tool(s).\")\n",
        "\n",
        "    # --- 2. Initial Model Call ---\n",
        "    print(\"Making initial call to Gemini model...\")\n",
        "    current_temperature = initial_temperature\n",
        "    response = await client.aio.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,  # Send updated history\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=1.0,\n",
        "                tools=[gemini_tool_config],\n",
        "            ),  # Keep sending same config\n",
        "        )\n",
        "    print(\"Initial response received.\")\n",
        "\n",
        "    # Append the model's first response (potentially including function calls) to history\n",
        "    # Need to handle potential lack of candidates or content\n",
        "    if not response.candidates:\n",
        "         print(\"Warning: Initial model response has no candidates.\")\n",
        "         # Decide how to handle this - raise error or return the empty response?\n",
        "         return response\n",
        "    contents.append(response.candidates[0].content)\n",
        "\n",
        "    # --- 3. Tool Calling Loop ---\n",
        "    turn_count = 0\n",
        "    # Check specifically for FunctionCall objects in the latest response part\n",
        "    latest_content = response.candidates[0].content\n",
        "    has_function_calls = any(part.function_call for part in latest_content.parts)\n",
        "\n",
        "    while has_function_calls and turn_count < max_tool_turns:\n",
        "        turn_count += 1\n",
        "        print(f\"\\n--- Tool Turn {turn_count}/{max_tool_turns} ---\")\n",
        "\n",
        "        # --- 3.1 Execute Pending Function Calls ---\n",
        "        function_calls_to_execute = [part.function_call for part in latest_content.parts if part.function_call]\n",
        "        tool_response_parts = await _execute_tool_calls(function_calls_to_execute, session)\n",
        "\n",
        "        # --- 3.2 Add Tool Responses to History ---\n",
        "        # Send back the results for *all* function calls from the previous turn\n",
        "        contents.append(types.Content(role=\"function\", parts=tool_response_parts)) # Use \"function\" role\n",
        "        print(f\"Added {len(tool_response_parts)} tool response part(s) to history.\")\n",
        "\n",
        "        # --- 3.3 Make Subsequent Model Call with Tool Responses ---\n",
        "        print(\"Making subsequent API call to Gemini with tool responses...\")\n",
        "        current_temperature = tool_call_temperature # Use different temp for follow-up\n",
        "        response = await client.aio.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,  # Send updated history\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=1.0,\n",
        "                tools=[gemini_tool_config],\n",
        "            ),  \n",
        "        )\n",
        "        print(\"Subsequent response received.\")\n",
        "\n",
        "        # --- 3.4 Append latest model response and check for more calls ---\n",
        "        if not response.candidates:\n",
        "            print(\"Warning: Subsequent model response has no candidates.\")\n",
        "            break # Exit loop if no candidates are returned\n",
        "        latest_content = response.candidates[0].content\n",
        "        contents.append(latest_content)\n",
        "        has_function_calls = any(part.function_call for part in latest_content.parts)\n",
        "        if not has_function_calls:\n",
        "            print(\"Model response contains text, no further tool calls requested this turn.\")\n",
        "\n",
        "\n",
        "    # --- 4. Loop Termination Check ---\n",
        "    if turn_count >= max_tool_turns and has_function_calls:\n",
        "        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop even though function calls might be pending.\")\n",
        "    elif not has_function_calls:\n",
        "        print(\"Tool calling loop finished naturally (model provided text response).\")\n",
        "\n",
        "    # --- 5. Return Final Response ---\n",
        "    print(\"Agent loop finished. Returning final response.\")\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1:  Create MCP Server for Google Cloud BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Please create an MCP server code for google cloud big query. It has two tools. One is to list tables for all datasets, the other is to describe a table. Google cloud project id and location will be provided for use. please use project id to access big query client.\n",
        "  \n",
        "  Return Python code within a JSON object formatted exactly as: {'python_code':    'your_generated_code', 'description':'your description'}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_text= generate_mcp_server(prompt)\n",
        "python_code=json.loads(response_text)['python_code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully formatted the code and saved it to 'server/bq.py'\n"
          ]
        }
      ],
      "source": [
        "format_python(python_code, \"server/bq.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "bq_server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    # Make sure to update to the full absolute path to your server file\n",
        "    args=[\"./server/bq.py\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'\n",
            "Starting agent loop with model 'gemini-2.5-pro-exp-03-25' and prompt: 'Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'...'\n",
            "Initializing session...\n",
            "Listing tools from session...\n",
            "Configured Gemini with 2 tool(s).\n",
            "Making initial call to Gemini model...\n",
            "Initial response received.\n",
            "\n",
            "--- Tool Turn 1/5 ---\n",
            "--- Executing 1 tool call(s) ---\n",
            "  Attempting to call session tool: 'list_tables' with args: {'project_id': 'dw-genai-dev'}\n",
            "  Session tool 'list_tables' execution finished.\n",
            "  Tool 'list_tables' succeeded. Result snippet: demo_dataset1.item_table\n",
            "demo_dataset1.user_table\n",
            "demo_dataset2.item_table\n",
            "demo_dataset2.user_table...\n",
            "--- Finished executing tool call(s) ---\n",
            "Added 1 tool response part(s) to history.\n",
            "Making subsequent API call to Gemini with tool responses...\n",
            "Subsequent response received.\n",
            "Model response contains text, no further tool calls requested this turn.\n",
            "Tool calling loop finished naturally (model provided text response).\n",
            "Agent loop finished. Returning final response.\n",
            "Okay, I found the following tables in the project 'dw-genai-dev':\n",
            "\n",
            "*   demo_dataset1.item_table\n",
            "*   demo_dataset1.user_table\n",
            "*   demo_dataset2.item_table\n",
            "*   demo_dataset2.user_table\n"
          ]
        }
      ],
      "source": [
        "async def run():\n",
        "    async with stdio_client(bq_server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await run_agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2:  Create MCP server for Medlineplus website\n",
        "Create an MCP server for \n",
        "https://medlineplus.gov/about/developers/webservices/ API service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "med_url =\"https://medlineplus.gov/about/developers/webservices/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched content\n"
          ]
        }
      ],
      "source": [
        "med_api_details = get_url_content(med_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully formatted the code and saved it to 'server/med.py'\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "  Please create an MCP server code for https://medlineplus.gov/about/developers/webservices/. It has one tool, get_medical_term. You provide a medical term, this tool will return explanation of the medial term\n",
        "  \n",
        "  Here's the API details:\n",
        "  {med_api_details}\n",
        "\"\"\"\n",
        "\n",
        "response_text= generate_mcp_server(prompt)\n",
        "python_code=json.loads(response_text)['python_code']\n",
        "\n",
        "format_python(python_code, \"server/med.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "med_server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    # Make sure to update to the full absolute path to your server file\n",
        "    args=[\"./server/med.py\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: Please explain flu in detail.\n",
            "Starting agent loop with model 'gemini-2.5-pro-exp-03-25' and prompt: 'Please explain flu in detail....'\n",
            "Initializing session...\n",
            "Listing tools from session...\n",
            "Configured Gemini with 1 tool(s).\n",
            "Making initial call to Gemini model...\n",
            "Initial response received.\n",
            "\n",
            "--- Tool Turn 1/5 ---\n",
            "--- Executing 1 tool call(s) ---\n",
            "  Attempting to call session tool: 'get_medical_term' with args: {'medical_term': 'flu'}\n",
            "  Session tool 'get_medical_term' execution finished.\n",
            "  Tool 'get_medical_term' succeeded. Result snippet: Term: <span class=\"qt0\">Flu</span>\n",
            "URL: https://medlineplus.gov/flu.html\n",
            "\n",
            "Summary:\n",
            "What is the <span class=\"qt0\">flu</span>?<p>The <span class=\"qt0\">f...\n",
            "--- Finished executing tool call(s) ---\n",
            "Added 1 tool response part(s) to history.\n",
            "Making subsequent API call to Gemini with tool responses...\n",
            "Subsequent response received.\n",
            "Model response contains text, no further tool calls requested this turn.\n",
            "Tool calling loop finished naturally (model provided text response).\n",
            "Agent loop finished. Returning final response.\n",
            "Okay, here is a detailed explanation of the flu (influenza) based on information from MedlinePlus:\n",
            "\n",
            "**What is the flu?**\n",
            "The flu, also called influenza, is a respiratory infection caused by viruses. Millions of Americans get sick with the flu each year. While it often causes mild illness, it can sometimes be serious or even deadly, particularly for individuals over 65, newborn babies, and those with certain chronic illnesses.\n",
            "\n",
            "**What causes the flu?**\n",
            "The flu is caused by influenza viruses that spread from person to person. When an infected person coughs, sneezes, or talks, they release tiny droplets containing the virus. These droplets can be inhaled by people nearby. Less commonly, infection can occur by touching a surface or object contaminated with the flu virus and then touching one's own mouth, nose, or possibly eyes.\n",
            "\n",
            "**What are the symptoms of the flu?**\n",
            "Flu symptoms typically appear suddenly and may include:\n",
            "*   Fever or feeling feverish/chills\n",
            "*   Cough\n",
            "*   Sore throat\n",
            "*   Runny or stuffy nose\n",
            "*   Muscle or body aches\n",
            "*   Headaches\n",
            "*   Fatigue (tiredness)\n",
            "*   Some people, especially children, may also experience vomiting and diarrhea.\n",
            "\n",
            "**Flu vs. Cold:**\n",
            "It can sometimes be hard to tell the difference between a cold and the flu:\n",
            "*   **Onset:** Flu symptoms usually start suddenly, while cold symptoms develop slowly.\n",
            "*   **Fever:** Fever is usual with the flu but rare with a cold.\n",
            "*   **Aches:** Body aches are common and often significant with the flu, but usually slight with a cold.\n",
            "*   **Fatigue:** Fatigue and weakness are usual with the flu but only sometimes present with a cold.\n",
            "*   **Headache:** Headaches are common with the flu but rare with a cold.\n",
            "*   **Stuffy nose, sneezing, sore throat:** These are common with a cold but only sometimes occur with the flu.\n",
            "\n",
            "*(Note: \"Stomach flu\" is a common term but refers to gastroenteritis, not influenza.)*\n",
            "\n",
            "**What other problems can the flu cause?**\n",
            "Some individuals may develop complications, which can be serious or life-threatening. These include:\n",
            "*   Bronchitis\n",
            "*   Ear infection\n",
            "*   Sinus infection\n",
            "*   Pneumonia\n",
            "*   Inflammation of the heart (myocarditis), brain (encephalitis), or muscle tissues (myositis, rhabdomyolysis).\n",
            "\n",
            "The flu can also worsen chronic health problems, such as triggering asthma attacks in people with asthma.\n",
            "\n",
            "**Who is at higher risk for complications?**\n",
            "*   Adults 65 and older\n",
            "*   Pregnant women\n",
            "*   Children younger than 5\n",
            "*   People with certain chronic health conditions (e.g., asthma, diabetes, heart disease)\n",
            "\n",
            "**How is the flu diagnosed?**\n",
            "Diagnosis usually involves a medical history and review of symptoms. Tests can confirm the diagnosis using a swab from the inside of the nose or the back of the throat. Rapid tests provide results in 15-20 minutes but are less accurate than other tests that take one to several hours.\n",
            "\n",
            "**What are the treatments for the flu?**\n",
            "Most people recover on their own with rest and fluids. It's important to stay home and avoid contact with others (except for medical care) to prevent spreading the virus.\n",
            "\n",
            "If you have flu symptoms and are in a high-risk group, or if you are very sick or concerned, contact your healthcare provider. Antiviral medicines may be prescribed. These drugs can lessen the severity and duration of the illness and help prevent serious complications, especially if started within 2 days of symptom onset.\n",
            "\n",
            "**Can the flu be prevented?**\n",
            "*   **Vaccination:** The best way to prevent the flu is to get an annual flu vaccine.\n",
            "*   **Good hygiene:** Practicing good health habits like covering your cough, washing your hands often, and avoiding close contact with sick people can help stop the spread of germs.\n"
          ]
        }
      ],
      "source": [
        "async def run():\n",
        "    async with stdio_client(med_server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"Please explain flu in detail.\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await run_agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Create MCP Server for NIH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "nih_url=\"https://clinicaltables.nlm.nih.gov/apidoc/icd10cm/v3/doc.html\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully fetched content\n"
          ]
        }
      ],
      "source": [
        "nih_api_details = get_url_content(nih_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully formatted the code and saved it to 'server/nih.py'\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "  Please create an MCP server code for NIH. It has one tool, get_icd_10_code. You provide a name or code, it will return top 5 results. \n",
        "  \n",
        "  Here's the API details:\n",
        "   {nih_api_details}\n",
        "\"\"\"\n",
        "\n",
        "response_text= generate_mcp_server(prompt)\n",
        "python_code=json.loads(response_text)['python_code']\n",
        "\n",
        "format_python(python_code, \"server/nih.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: Please tell me ICD_10 code for Tuberculosis\n",
            "Starting agent loop with model 'gemini-2.5-pro-exp-03-25' and prompt: 'Please tell me ICD_10 code for Tuberculosis...'\n",
            "Initializing session...\n",
            "Listing tools from session...\n",
            "Configured Gemini with 1 tool(s).\n",
            "Making initial call to Gemini model...\n",
            "Initial response received.\n",
            "\n",
            "--- Tool Turn 1/5 ---\n",
            "--- Executing 1 tool call(s) ---\n",
            "  Attempting to call session tool: 'get_icd_10_code' with args: {'query': 'Tuberculosis'}\n",
            "  Session tool 'get_icd_10_code' execution finished.\n",
            "  Tool 'get_icd_10_code' succeeded. Result snippet: Code: A18.39, Name: Retroperitoneal tuberculosis\n",
            "Code: P37.0, Name: Congenital tuberculosis\n",
            "Code: Z22.7, Name: Latent tuberculosis\n",
            "Code: A15.0, Name: ...\n",
            "--- Finished executing tool call(s) ---\n",
            "Added 1 tool response part(s) to history.\n",
            "Making subsequent API call to Gemini with tool responses...\n",
            "Subsequent response received.\n",
            "Model response contains text, no further tool calls requested this turn.\n",
            "Tool calling loop finished naturally (model provided text response).\n",
            "Agent loop finished. Returning final response.\n",
            "Okay, I found several ICD-10 codes related to Tuberculosis. The specific code depends on the type or location of the tuberculosis. Here are the top 5 results:\n",
            "\n",
            "1.  **A15.0**: Tuberculosis of lung\n",
            "2.  **A15.7**: Primary respiratory tuberculosis\n",
            "3.  **A18.39**: Retroperitoneal tuberculosis\n",
            "4.  **P37.0**: Congenital tuberculosis\n",
            "5.  **Z22.7**: Latent tuberculosis\n",
            "\n",
            "Could you provide more details about the specific type of tuberculosis you're interested in?\n"
          ]
        }
      ],
      "source": [
        "nih_server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    # Make sure to update to the full absolute path to your server file\n",
        "    args=[\"./server/nih.py\"],\n",
        ")\n",
        "\n",
        "async def run():\n",
        "    async with stdio_client(nih_server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"Please tell me ICD_10 code for Tuberculosis\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await run_agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook_template.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
